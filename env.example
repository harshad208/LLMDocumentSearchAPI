# LLM Document Search API - Example Environment Variables
# Copy this file to .env and fill in your actual values.

# Application Settings
APP_VERSION="0.3.0"
LOG_LEVEL="INFO"    # DEBUG, INFO, WARNING, ERROR, CRITICAL

# Upload Settings
UPLOAD_DIRECTORY="uploads" # Relative to project root

# Chunking Settings
CHUNK_SIZE=1000
CHUNK_OVERLAP=150

# Embedding Model Settings
EMBEDDING_MODEL_NAME="all-MiniLM-L6-v2"

# Vector Store Settings (ChromaDB)
CHROMA_DATA_PATH="chroma_data"          # Relative to project root
CHROMA_COLLECTION_NAME="document_chunks"

# LLM Settings
# IMPORTANT: Download a GGUF model and update this path.
# Example uses Phi-3-mini. Path is relative to project root.
LLM_MODEL_PATH="llm_models/Phi-3-mini-4k-instruct-Q4_K_M.gguf"
LLM_N_CTX=2048       # Context window size for the LLM
LLM_N_GPU_LAYERS=0   # Number of layers to offload to GPU (0 for CPU-only by default)
LLM_MAX_TOKENS=300   # Max tokens to generate in LLM response
LLM_TEMPERATURE=0.3  # LLM temperature (0.0-1.0, lower is more deterministic)

# API Server Settings (used by run.py or if uvicorn run via python main.py)
API_HOST="0.0.0.0"
API_PORT=8000